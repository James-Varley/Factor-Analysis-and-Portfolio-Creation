{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "pNtxAdfVkUTn",
        "outputId": "0d4fb7a8-7db8-44c7-bb57-177af37c68e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pypfopt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3183193750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpypfopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEfficientFrontier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrisk_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;31m# For a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelativedelta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrelativedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pypfopt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as web\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pypfopt import EfficientFrontier, risk_models\n",
        "from tqdm.notebook import tqdm # For a progress bar\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "-----------------------          Scrape Data          -----------------------\n",
        "'''\n",
        "\n",
        "today = date.today()\n",
        "formatted_date = today.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# tickers = ['NVDA', 'MSFT', 'AAPL', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'UNH', 'JNJ', 'V', 'WMT', 'JPM', 'MA', 'PG', 'HD', 'BAC', 'XOM', 'KO']   # tried using wikipedia scraper to get SMP500 companies but doesn't work due to limitations in yfinance data request limits\n",
        "tickers = ['NVDA', 'MSFT', 'AAPL', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO']\n",
        "start = '2015-01-01'\n",
        "end = formatted_date\n",
        "\n",
        "price_data_all = yf.download(tickers, start=start, end=end)\n",
        "if price_data_all.empty:\n",
        "    raise ValueError(f\"No data downloaded from yfinance for tickers: {tickers}.\")\n",
        "try:\n",
        "    if isinstance(tickers, str) or (isinstance(tickers, list) and len(tickers) == 1):\n",
        "        prices = price_data_all['Close']\n",
        "    else:\n",
        "        prices = price_data_all['Close']\n",
        "except KeyError:\n",
        "    print(\"Error: Could not find 'Close' column in downloaded data.\")\n",
        "    raise\n",
        "\n",
        "returns = prices.pct_change().dropna()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "-----------------------         Risk Free Rate         -----------------------\n",
        "'''\n",
        "\n",
        "rf = yf.download('^IRX', start=start, end=end)['Close'] / 100   # gets data for 3-year treasury bond\n",
        "rf = rf / 252   # converts anualized yield to daily yield\n",
        "rf = rf.reindex(returns.index).ffill()\n",
        "\n",
        "if isinstance(rf, pd.DataFrame):   # convert dataframe to series\n",
        "    rf = rf.iloc[:, 0]  # take the first column as a Series\n",
        "\n",
        "excess_returns = returns.sub(rf, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "-----------------------         Factor Returns         -----------------------\n",
        "'''\n",
        "\n",
        "factor_etfs = {   # define ETFs representing each factor\n",
        "    'MKT': 'SPY',   # market\n",
        "    'SMB': 'IJR',   # small-cap\n",
        "    'HML_val': 'IWD',  # value\n",
        "    'HML_gro': 'IWF',  # growth\n",
        "    'MOM': 'MTUM',  # momentum\n",
        "}\n",
        "\n",
        "factor_prices = yf.download(list(factor_etfs.values()), start=start, end=end, auto_adjust=True)['Close']   # download daily prices\n",
        "factor_returns = factor_prices.pct_change().dropna()   # compute daily returns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "factors = pd.DataFrame(index=factor_returns.index)   # construct approximate factors\n",
        "factors['MKT'] = factor_returns['SPY']\n",
        "factors['SMB'] = factor_returns['IJR'] - factor_returns['SPY']\n",
        "factors['HML'] = factor_returns['IWD'] - factor_returns['IWF']\n",
        "factors['MOM'] = factor_returns['MTUM'] - factor_returns['SPY']\n",
        "\n",
        "qual_returns = yf.download('QUAL', start=start, end=end, auto_adjust=True)['Close'].pct_change().squeeze()\n",
        "v_returns = yf.download('VTV', start=start, end=end, auto_adjust=True)['Close'].pct_change().squeeze()\n",
        "bk_returns = yf.download('VBK', start=start, end=end, auto_adjust=True)['Close'].pct_change().squeeze()\n",
        "qual_returns = qual_returns.reindex(factors.index).fillna(method='ffill')\n",
        "v_returns = v_returns.reindex(factors.index).fillna(method='ffill')\n",
        "bk_returns = bk_returns.reindex(factors.index).fillna(method='ffill')\n",
        "\n",
        "factors['RMW'] = qual_returns - factors['MKT']\n",
        "factors['CMA'] = v_returns - bk_returns\n",
        "\n",
        "factors = factors.ffill().dropna()\n",
        "excess_returns = excess_returns.reindex(factors.index).dropna(how='all')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "-----------------------          Merging data          -----------------------\n",
        "'''\n",
        "\n",
        "data = pd.concat([excess_returns, factors], axis=1, join='inner').dropna()   # removes any dates that dont exist in both datasets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "-----------------------           Regression           -----------------------\n",
        "'''\n",
        "\n",
        "models = {   # define factors for each model\n",
        "    \"CAPM\": [\"MKT\"],\n",
        "    \"FamaFrench3\": [\"MKT\", \"SMB\", \"HML\"],\n",
        "    \"Carhart4\": [\"MKT\", \"SMB\", \"HML\", \"MOM\"],\n",
        "    \"FamaFrench5\": [\"MKT\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
        "}\n",
        "\n",
        "window_months = [12, 24, 36, 48, 60]\n",
        "window_sizes = [m * 21 for m in window_months]  # convert months into trading days\n",
        "step_size = 21\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, factor_list in tqdm(models.items(), desc=\"Running Models\", leave=False):\n",
        "\n",
        "    model_results = {}\n",
        "\n",
        "    for window_size in tqdm(window_sizes, desc=f\"{model_name} Windows\", leave=False):\n",
        "\n",
        "        betas, alphas, r2s, tstats, dates = {}, {}, {}, {}, {}\n",
        "\n",
        "        for asset in tqdm(excess_returns.columns, desc=f\"{model_name} {window_size}-day\", leave=False):\n",
        "\n",
        "            y_full = excess_returns[asset].dropna().iloc[-window_size:]   # take last 'window_size' days of data for this asset\n",
        "            X_full = factors.loc[y_full.index, factor_list]\n",
        "            X_full = sm.add_constant(X_full)\n",
        "\n",
        "            betas[asset], alphas[asset], r2s[asset], tstats[asset], dates[asset] = [], [], [], [], []\n",
        "\n",
        "            for start in range(0, len(y_full) - step_size + 1, step_size):   # rolling regressions across y_full with window = step_size\n",
        "                end = start + step_size\n",
        "                y_window = y_full.iloc[start:end]\n",
        "                X_window = X_full.iloc[start:end]\n",
        "\n",
        "                try:\n",
        "                    model = sm.OLS(y_window, X_window).fit()\n",
        "                    alphas[asset].append(model.params['const'])\n",
        "                    betas[asset].append(model.params[factor_list])\n",
        "                    r2s[asset].append(model.rsquared)\n",
        "                    tstats[asset].append(model.tvalues['const'])\n",
        "                    dates[asset].append(y_window.index[-1])\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        model_results[window_size] = {\n",
        "            'alphas': alphas,\n",
        "            'betas': betas,\n",
        "            'r2s': r2s,\n",
        "            'tstats': tstats,\n",
        "            'dates': dates\n",
        "        }\n",
        "\n",
        "    results[model_name] = model_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "------------------       Aggrigate Performance Metrics       ------------------\n",
        "'''\n",
        "\n",
        "\n",
        "summary_stats = []\n",
        "\n",
        "for model_name, model_data in results.items():  # loop over models\n",
        "    for window, metrics in model_data.items():  # loop over window sizes\n",
        "\n",
        "        any_asset = next(iter(metrics['alphas']))   # determine the number of rolling periods (assuming all assets aligned)\n",
        "        num_windows = len(metrics['alphas'][any_asset])\n",
        "        if num_windows == 0:\n",
        "            continue\n",
        "\n",
        "        rolling_alphas = np.zeros((len(metrics['alphas']), num_windows))   # initialise arrays to collect rolling metrics across assets\n",
        "        rolling_r2s = np.zeros((len(metrics['alphas']), num_windows))\n",
        "\n",
        "        asset_list = list(metrics['alphas'].keys())\n",
        "        for i, asset in enumerate(asset_list):\n",
        "            alphas = np.array(metrics['alphas'][asset])\n",
        "            r2s = np.array(metrics['r2s'][asset])\n",
        "\n",
        "            if len(alphas) < num_windows:   # if asset has fewer windows, pad with NaN\n",
        "                alphas = np.pad(alphas, (0, num_windows - len(alphas)), constant_values=np.nan)\n",
        "                r2s = np.pad(r2s, (0, num_windows - len(r2s)), constant_values=np.nan)\n",
        "\n",
        "            rolling_alphas[i, :] = alphas\n",
        "            rolling_r2s[i, :] = r2s\n",
        "\n",
        "        mean_alpha_per_window = np.nanmean(rolling_alphas, axis=0)   # compute metrics per rolling window across assets\n",
        "        std_alpha_per_window = np.nanstd(rolling_alphas, axis=0, ddof=1)\n",
        "        t_alpha_per_window = np.where(std_alpha_per_window > 0,\n",
        "                                      mean_alpha_per_window / (std_alpha_per_window / np.sqrt(len(asset_list))),\n",
        "                                      np.nan)\n",
        "        mean_r2_per_window = np.nanmean(rolling_r2s, axis=0)\n",
        "\n",
        "        vols = []   # volatility & sharpe across assets\n",
        "        sharpes = []\n",
        "        for asset in asset_list:\n",
        "            if asset in excess_returns.columns:\n",
        "                ret = excess_returns[asset]\n",
        "                vol = ret.std()\n",
        "                mean_ret = ret.mean()\n",
        "                sharpe = mean_ret / vol if vol > 0 else np.nan\n",
        "                vols.append(vol)\n",
        "                sharpes.append(sharpe)\n",
        "\n",
        "        avg_vol = np.nanmean(vols)\n",
        "        avg_sharpe = np.nanmean(sharpes)\n",
        "\n",
        "        summary_stats.append({\n",
        "            'Model': model_name,\n",
        "            'Window (days)': window,\n",
        "            'Mean Alpha': np.nanmean(mean_alpha_per_window),\n",
        "            't(Alpha)': np.nanmean(t_alpha_per_window),\n",
        "            'Mean RÂ²': np.nanmean(mean_r2_per_window),\n",
        "            'Avg Volatility': avg_vol,\n",
        "            'Avg Sharpe': avg_sharpe,\n",
        "            'Num Assets': len(asset_list)\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "---------------------       Portfolio Construction       ---------------------\n",
        "'''\n",
        "\n",
        "\n",
        "user_start = input(\"Enter the start date for backtesting performance comparison (YYYY-MM-DD): \")   # timeframe to backtest portfolios over\n",
        "user_start = pd.to_datetime(user_start)\n",
        "\n",
        "portfolio_returns = {}\n",
        "portfolio_stats = []\n",
        "\n",
        "for model_name, model_data in results.items():   # loop over each model\n",
        "    print(f\"\\nBuilding portfolios for {model_name}...\")\n",
        "    for window, metrics in model_data.items():   # loop over each window\n",
        "        print(f\"  - Sampling Window: {window/21} months\")\n",
        "\n",
        "        mean_alphas = {   # mean alpha for each asset\n",
        "            asset: np.mean(metrics['alphas'][asset])\n",
        "            for asset in metrics['alphas']\n",
        "            if len(metrics['alphas'][asset]) > 0\n",
        "        }\n",
        "        if len(mean_alphas) == 0:\n",
        "            continue\n",
        "\n",
        "        abs_sum = np.sum(np.abs(list(mean_alphas.values())))   #normalizing weights (to give proper portfolio asset weightings)\n",
        "        weights = {asset: alpha / abs_sum for asset, alpha in mean_alphas.items()}\n",
        "\n",
        "        common_assets = list(set(weights.keys()).intersection(excess_returns.columns))\n",
        "        if not common_assets:\n",
        "            continue\n",
        "        sub_returns = excess_returns[common_assets]\n",
        "\n",
        "        port_ret = (sub_returns * pd.Series(weights)).sum(axis=1)   # multiply return of asset by its weight to get portfolio return for that asset\n",
        "        port_name = f\"{model_name}_{window}d\"\n",
        "        portfolio_returns[port_name] = port_ret\n",
        "\n",
        "        port_ret_window = port_ret.loc[port_ret.index >= user_start]   # statistics for defined backtesting window\n",
        "        if len(port_ret_window) == 0:\n",
        "            continue\n",
        "\n",
        "        cumulative = (1 + port_ret_window).cumprod()\n",
        "        total_return = cumulative.iloc[-1] - 1\n",
        "        sharpe = np.sqrt(252) * port_ret_window.mean() / port_ret_window.std() if port_ret_window.std() > 0 else np.nan\n",
        "        vol = np.sqrt(252) * port_ret_window.std()\n",
        "\n",
        "        portfolio_stats.append({\n",
        "            \"Portfolio\": port_name,\n",
        "            \"Total Return\": total_return,\n",
        "            \"Sharpe Ratio\": sharpe,\n",
        "            \"Volatility\": vol,\n",
        "            \"Num Assets\": len(weights)\n",
        "        })\n",
        "\n",
        "portfolio_stats_df = pd.DataFrame(portfolio_stats).sort_values(by=\"Total Return\", ascending=False)   # create a summary dataframe and get top 5\n",
        "top5 = portfolio_stats_df.head(5)\n",
        "print(\"\\nTop 5 Portfolios by Total Return since: \", user_start, \" \\n\")\n",
        "display(top5)\n",
        "\n",
        "plt.figure(figsize=(12, 6))   # plot top 5 portfolio returns\n",
        "for port_name in top5[\"Portfolio\"]:\n",
        "    port_ret_window = portfolio_returns[port_name].loc[portfolio_returns[port_name].index >= user_start]    # slice to user-defined timeframe\n",
        "    cum = (1 + port_ret_window).cumprod()\n",
        "    plt.plot(cum.index, cum, label=port_name)\n",
        "\n",
        "plt.title(f\"Top 5 Portfolio Cumulative Returns (Start $1)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Portfolio Value ($)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "top5_weights = {}   # display top 5 portfolio weights\n",
        "for port_name in top5[\"Portfolio\"]:\n",
        "    model_name, window = port_name.split(\"_\")\n",
        "    window = int(window.replace(\"d\", \"\"))\n",
        "    metrics = results[model_name][window]\n",
        "    mean_alphas = {\n",
        "        asset: np.mean(metrics[\"alphas\"][asset])\n",
        "        for asset in metrics[\"alphas\"]\n",
        "        if len(metrics[\"alphas\"][asset]) > 0\n",
        "    }\n",
        "    abs_sum = np.sum(np.abs(list(mean_alphas.values())))\n",
        "    weights = {asset: alpha / abs_sum for asset, alpha in mean_alphas.items()}\n",
        "    top5_weights[port_name] = weights\n",
        "\n",
        "weights_df = pd.DataFrame(top5_weights).fillna(0)\n",
        "print(\"\\nPortfolio Weights:\\n\")\n",
        "display(weights_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeCE_p3qlYa7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}